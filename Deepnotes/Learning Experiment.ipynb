{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "f10d08c7-e8af-4a05-863a-2e8009fb8ea8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "746d5b98",
    "execution_start": 1656863357420,
    "execution_millis": 2849,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 442
   },
   "source": "from process_handler import Process, ProcessManager\nfrom bytes_tokenizer import encode, decode, tokenizer\nfrom transformer_model import create_model, create_mask\nfrom text_dataset import TextDataset\nfrom checkpoint_manager import *\nfrom train_eval_utils import *\nfrom modeling_utils import *\n\nimport numpy as np\nimport time\nimport torch\nimport os\nimport shutil\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Optimizer\nfrom tqdm import tqdm, trange\nfrom typing import List, Tuple, Dict, Any\nfrom torch.nn import Module",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "class LearningExperiment:\n    def __init__(self, model: Module, \n                 model_params: Dict[str, Any], \n                 optimizer_function: str = 'adam', \n                 criterion_function: str = 'cross-entropy', \n                 learning_rate: float = 1e-3, \n                 device: str = 'cpu', \n                 checkpoint_dir: str = 'checkpoint'):\n        self.model = model\n        self.model_params = model_params\n        self.optimizer_function = optimizer_function\n        self.criterion_function = criterion_function\n        self.learning_rate = learning_rate\n        self.device = device\n        self.checkpoint_dir = checkpoint_dir\n\n        self.optimizer: Optimizer = None\n        self.criterion: nn.Module = None\n\n    def setup(self):\n        self.optimizer = get_optimizer(self.model, self.optimizer_function, self.learning_rate)\n        self.criterion = get_criterion(self.criterion_function)\n\n    def load_checkpoint(self, checkpoint: str):\n        load_checkpoint(checkpoint, self.model, self.optimizer)\n\n    def save_checkpoint(self, is_best: bool, checkpoint: str):\n        state = {\n            'state_dict': self.model.state_dict(),\n            'optim_dict': self.optimizer.state_dict()\n        }\n        save_checkpoint(state, is_best, checkpoint)\n\n    def train(self, train_dataset, batch_size: int = 32, epochs: int = 100):\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n        for epoch in range(1, epochs + 1):\n            avg_loss = train(self.model, train_loader, self.optimizer, self.criterion, self.device)\n            print('[Epoch {}] Train loss: {:.4f}'.format(epoch, avg_loss))\n\n    def evaluate(self, test_dataset, batch_size: int = 32):\n        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n        avg_loss = evaluate(self.model, test_loader, self.criterion, self.device)\n        print('Test loss: {:.4f}'.format(avg_loss))\n\n    def train_and_evaluate(self, train_dataset: TextDataset, \n                           test_dataset: TextDataset, \n                           batch_size: int = 32, \n                           epochs: int = 100):\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n        for epoch in range(1, epochs + 1):\n            train_avg_loss = train(self.model, train_loader, self.optimizer, self.criterion, self.device)\n            test_avg_loss = evaluate(self.model, test_loader, self.criterion, self.device)\n            print('[Epoch {}] Train loss: {:.4f} | Test loss: {:.4f}'.format(epoch, train_avg_loss, test_avg_loss))\n\n    def run(self, train_dataset: TextDataset, test_dataset: TextDataset, batch_size: int = 32, epochs: int = 100):\n        self.setup()\n        self.train_and_evaluate(train_dataset, test_dataset, batch_size=batch_size, epochs=epochs)\n\n        is_best = True\n        self.save_checkpoint(is_best, self.checkpoint_dir)\n\n    def run_from_checkpoint(self, train_dataset: TextDataset, test_dataset: TextDataset, checkpoint: str, batch_size: int = 32, epochs: int = 100):\n        self.setup()\n        self.load_checkpoint(checkpoint)\n        self.train_and_evaluate(train_dataset, test_dataset, batch_size=batch_size, epochs=epochs)\n\n        is_best = True\n        self.save_checkpoint(is_best, self.checkpoint_dir)\n\n    def run_experiment(self, train_dataset: TextDataset, test_dataset: TextDataset, batch_size: int = 32, epochs: int = 100):\n        self.setup()\n        if os.path.exists(self.checkpoint_dir):\n            self.run_from_checkpoint(train_dataset, test_dataset, checkpoint=os.path.join(self.checkpoint_dir, 'last.pth.tar'), batch_size=batch_size, epochs=epochs)\n        else:\n            self.run(train_dataset, test_dataset, batch_size=batch_size, epochs=epochs)",
   "metadata": {
    "cell_id": "4e8770bf991b40ea8a274f91ccc057eb",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "91723a72",
    "execution_start": 1656863360285,
    "execution_millis": 38,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1485
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "train_dataset = TextDataset(os.path.join('data', 'train_dataset.txt'), max_len=100)\ntest_dataset = TextDataset(os.path.join('data', 'test_dataset.txt'), max_len=100)\nmodel = create_model(ntoken=len(tokenizer), ninp=512, nhead=1, nhid=1024, nlayers=6, device='cpu', dropout=0.5)\nexp = LearningExperiment(model, model_params={}, optimizer_function='adam', criterion_function='cross-entropy', learning_rate=1e-3, device='cpu', checkpoint_dir='checkpoint')\nexp.run_experiment(train_dataset, test_dataset, batch_size=64, epochs=100)",
   "metadata": {
    "cell_id": "6a30f201351a48c383e795beac5ee68f",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b24b3c8d",
    "execution_start": 1656863360341,
    "execution_millis": 321,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 588,
    "deepnote_output_heights": [
     192,
     40
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/root/work/bytes_tokenizer.py:45: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n  return torch.from_numpy(encoded_string)\n 33%|███▎      | 1/3 [00:08<00:17,  8.93s/it]Total Loss: 6.154289245605469\n 67%|██████▋   | 2/3 [00:16<00:08,  8.12s/it]Total Loss: 11.515909671783447\n100%|██████████| 3/3 [00:19<00:00,  6.57s/it]\nTotal Loss: 14.134660720825195\n100%|██████████| 3/3 [00:05<00:00,  1.94s/it]\n[Epoch 1] Train loss: 4.7116 | Test loss: 4.5192\n 33%|███▎      | 1/3 [00:06<00:13,  6.90s/it]Total Loss: 3.720729351043701\n 67%|██████▋   | 2/3 [00:14<00:07,  7.17s/it]Total Loss: 7.1338889598846436\n100%|██████████| 3/3 [00:16<00:00,  5.66s/it]\nTotal Loss: 9.545804500579834\n100%|██████████| 3/3 [00:04<00:00,  1.54s/it]\n[Epoch 2] Train loss: 3.1819 | Test loss: 2.2000\n 33%|███▎      | 1/3 [00:06<00:12,  6.20s/it]Total Loss: 2.014049768447876\n 67%|██████▋   | 2/3 [00:13<00:06,  6.70s/it]Total Loss: 4.454030990600586\n100%|██████████| 3/3 [00:15<00:00,  5.32s/it]\nTotal Loss: 6.668410778045654\n100%|██████████| 3/3 [00:04<00:00,  1.60s/it]\n[Epoch 3] Train loss: 2.2228 | Test loss: 1.8804\n 33%|███▎      | 1/3 [00:06<00:12,  6.39s/it]Total Loss: 1.9011412858963013\n 67%|██████▋   | 2/3 [00:13<00:06,  6.59s/it]Total Loss: 3.8482506275177\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c8418618-5b01-4dd8-b931-34351753cb66' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "0b63dfd2-ee57-43d7-b151-56a2da498357",
  "deepnote_execution_queue": [
   {
    "cellId": "6a30f201351a48c383e795beac5ee68f",
    "sessionId": "def5d2d8-289e-4e95-becb-72baa8bfc911",
    "msgId": "e9780bed-b562-4e01-ac26-39d88bd76187"
   }
  ]
 }
}